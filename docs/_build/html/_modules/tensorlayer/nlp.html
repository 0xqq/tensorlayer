

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorlayer.nlp &mdash; TensorLayer 1.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="TensorLayer 1.1 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> TensorLayer
          

          
          </a>

          
            
            
              <div class="version">
                1.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../user/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user/tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user/development.html">Development</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../modules/layers.html"><code class="docutils literal"><span class="pre">tensorlayer.layers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/activation.html"><code class="docutils literal"><span class="pre">tensorlayer.activation</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/nlp.html"><code class="docutils literal"><span class="pre">tensorlayer.nlp</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/rein.html"><code class="docutils literal"><span class="pre">tensorlayer.rein</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/iterate.html"><code class="docutils literal"><span class="pre">tensorlayer.iterate</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/cost.html"><code class="docutils literal"><span class="pre">tensorlayer.cost</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/visualize.html"><code class="docutils literal"><span class="pre">tensorlayer.visualize</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/files.html"><code class="docutils literal"><span class="pre">tensorlayer.files</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/utils.html"><code class="docutils literal"><span class="pre">tensorlayer.utils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/init.html"><code class="docutils literal"><span class="pre">tensorlayer.init</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/preprocess.html"><code class="docutils literal"><span class="pre">tensorlayer.preprocess</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../modules/ops.html"><code class="docutils literal"><span class="pre">tensorlayer.ops</span></code></a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">TensorLayer</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">Module code</a> &raquo;</li>
      
    <li>tensorlayer.nlp</li>
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorlayer.nlp</h1><div class="highlight"><pre>
<span></span><span class="ch">#! /usr/bin/python</span>
<span class="c1"># -*- coding: utf8 -*-</span>




<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">platform</span> <span class="k">as</span> <span class="n">_platform</span>
<span class="kn">from</span> <span class="nn">.layers</span> <span class="kn">import</span> <span class="n">set_keep</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">xrange</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">gfile</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1">## Iteration functions</span>
<div class="viewcode-block" id="generate_skip_gram_batch"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.generate_skip_gram_batch">[docs]</a><span class="k">def</span> <span class="nf">generate_skip_gram_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_skips</span><span class="p">,</span> <span class="n">skip_window</span><span class="p">,</span> <span class="n">data_index</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate a training batch for the Skip-Gram model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : a list</span>
<span class="sd">        To present context.</span>
<span class="sd">    batch_size : an int</span>
<span class="sd">        Batch size to return.</span>
<span class="sd">    num_skips : an int</span>
<span class="sd">        How many times to reuse an input to generate a label.</span>
<span class="sd">    skip_window : an int</span>
<span class="sd">        How many words to consider left and right.</span>
<span class="sd">    data_index : an int</span>
<span class="sd">        Index of the context location.</span>
<span class="sd">        without using yield, this code use data_index to instead.</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    batch : a list</span>
<span class="sd">        Inputs</span>
<span class="sd">    labels : a list</span>
<span class="sd">        Labels</span>
<span class="sd">    data_index : an int</span>
<span class="sd">        Index of the context location.</span>

<span class="sd">    Example</span>
<span class="sd">    --------</span>
<span class="sd">    Setting num_skips=2, skip_window=1, use the right and left words.</span>
<span class="sd">    In the same way, num_skips=4, skip_window=2 means use the nearby 4 words.</span>

<span class="sd">    &gt;&gt;&gt; data = [1,2,3,4,5,6,7,8,9,10,11]</span>
<span class="sd">    &gt;&gt;&gt; batch, labels, data_index = tl.nlp.generate_skip_gram_batch(    \</span>
<span class="sd">        data=data, batch_size=8, num_skips=2, skip_window=1, data_index=0)</span>
<span class="sd">    &gt;&gt;&gt; print(batch)</span>
<span class="sd">    ... [2 2 3 3 4 4 5 5]</span>
<span class="sd">    &gt;&gt;&gt; print(labels)</span>
<span class="sd">    ... [[3]</span>
<span class="sd">    ... [1]</span>
<span class="sd">    ... [4]</span>
<span class="sd">    ... [2]</span>
<span class="sd">    ... [5]</span>
<span class="sd">    ... [3]</span>
<span class="sd">    ... [4]</span>
<span class="sd">    ... [6]]</span>

<span class="sd">    References</span>
<span class="sd">    -----------</span>
<span class="sd">    `TensorFlow word2vec tutorial &lt;https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#vector-representations-of-words&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># global data_index   # you can put data_index outside the function, then</span>
    <span class="c1">#       modify the global data_index in the function without return it.</span>
    <span class="c1"># note: without using yield, this code use data_index to instead.</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">num_skips</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">num_skips</span> <span class="o">&lt;=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">span</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">skip_window</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># [ skip_window target skip_window ]</span>
    <span class="nb">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">span</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">span</span><span class="p">):</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">num_skips</span><span class="p">):</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">skip_window</span>  <span class="c1"># target label at the center of the buffer</span>
        <span class="n">targets_to_avoid</span> <span class="o">=</span> <span class="p">[</span> <span class="n">skip_window</span> <span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_skips</span><span class="p">):</span>
            <span class="k">while</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets_to_avoid</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">span</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">targets_to_avoid</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">skip_window</span><span class="p">]</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_skips</span> <span class="o">+</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">buffer</span><span class="p">[</span><span class="n">target</span><span class="p">]</span>
        <span class="nb">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">data_index</span><span class="p">])</span>
        <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">data_index</span></div>


<span class="c1">## Sampling functions</span>
<div class="viewcode-block" id="sample"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.sample">[docs]</a><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sample an index from a probability array.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    a : a list</span>
<span class="sd">        List of probabilities.</span>
<span class="sd">    temperature : float or None</span>
<span class="sd">        The higher the more uniform.</span>
<span class="sd">        When a = [0.1, 0.2, 0.7],</span>
<span class="sd">            temperature = 0.7, the distribution will be sharpen [ 0.05048273  0.13588945  0.81362782]</span>
<span class="sd">            temperature = 1.0, the distribution will be the same [0.1    0.2    0.7]</span>
<span class="sd">            temperature = 1.5, the distribution will be filtered [ 0.16008435  0.25411807  0.58579758]</span>
<span class="sd">        If None, it will be ``np.argmax(a)``</span>

<span class="sd">    Note</span>
<span class="sd">    ------</span>
<span class="sd">    No matter what is the temperature and input list, the sum of all probabilities will be one.</span>
<span class="sd">    Even if input list = [1, 100, 200], the sum of all probabilities will still be one.</span>

<span class="sd">    For large vocabulary_size, choice a higher temperature to avoid error.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="c1"># np.set_printoptions(threshold=np.nan)</span>
        <span class="c1"># print(a)</span>
        <span class="c1"># print(np.sum(a))</span>
        <span class="c1"># print(np.max(a))</span>
        <span class="c1"># print(np.min(a))</span>
        <span class="c1"># exit()</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;For large vocabulary_size, choice a higher temperature</span><span class="se">\</span>
<span class="s2">         to avoid log error. Hint : use ``sample_top``. &quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="ne">Warning</span><span class="p">)</span>
        <span class="c1"># print(a)</span>
        <span class="c1"># print(b)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></div>

<div class="viewcode-block" id="sample_top"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.sample_top">[docs]</a><span class="k">def</span> <span class="nf">sample_top</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sample from ``top_k`` probabilities.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">a</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="n">top_k</span><span class="p">]</span>
    <span class="c1"># a = a[idx]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">choice</span></div>


<span class="c1">## Vector representations of words</span>
<div class="viewcode-block" id="simple_read_words"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.simple_read_words">[docs]</a><span class="k">def</span> <span class="nf">simple_read_words</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;nietzsche.txt&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Read context from file without any preprocessing.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;nietzsche.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">words</span></div>

<div class="viewcode-block" id="read_words"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.read_words">[docs]</a><span class="k">def</span> <span class="nf">read_words</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">replace</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&lt;eos&gt;&#39;</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;File to list format context.</span>
<span class="sd">    Note that: this script can not handle punctuations.</span>
<span class="sd">    For customized read_words method, see ``tutorial_generate_text.py``.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    filename : a string</span>
<span class="sd">        A file path (like .txt file),</span>
<span class="sd">    replace : a list</span>
<span class="sd">        [original string, target string], to disable replace use [&#39;&#39;, &#39;&#39;]</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    The context in a list, split by &#39; &#39; by default, and use &#39;&lt;eos&gt;&#39; to represent &#39;\n&#39;.</span>
<span class="sd">    e.g. [... &#39;how&#39;, &#39;useful&#39;, &#39;it&#39;, &quot;&#39;s&quot; ... ]</span>

<span class="sd">    Code References</span>
<span class="sd">    ---------------</span>
<span class="sd">    `tensorflow.models.rnn.ptb.reader &lt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb&gt;`_</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="o">*</span><span class="n">replace</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span></div>

<div class="viewcode-block" id="read_analogies_file"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.read_analogies_file">[docs]</a><span class="k">def</span> <span class="nf">read_analogies_file</span><span class="p">(</span><span class="n">eval_file</span><span class="o">=</span><span class="s1">&#39;questions-words.txt&#39;</span><span class="p">,</span> <span class="n">word2id</span><span class="o">=</span><span class="p">{}):</span>
    <span class="sd">&quot;&quot;&quot;Reads through an analogy question file, return its id format.</span>

<span class="sd">    eval_data : a string</span>
<span class="sd">        The file name.</span>
<span class="sd">    word2id : a dictionary</span>
<span class="sd">        Mapping words to unique IDs.</span>
<span class="sd">    Returns:</span>
<span class="sd">    questions: a [n, 4] numpy array containing the analogy question&#39;s</span>
<span class="sd">             word ids.</span>
<span class="sd">             questions_skipped: questions skipped due to unknown words.</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; eval_file should be in this format :</span>
<span class="sd">    &gt;&gt;&gt; : capital-common-countries</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Baghdad Iraq</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Bangkok Thailand</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Beijing China</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Berlin Germany</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Bern Switzerland</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Cairo Egypt</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Canberra Australia</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Hanoi Vietnam</span>
<span class="sd">    &gt;&gt;&gt; Athens Greece Havana Cuba</span>
<span class="sd">    ...</span>

<span class="sd">    &gt;&gt;&gt; words = tl.files.load_matt_mahoney_text8_dataset()</span>
<span class="sd">    &gt;&gt;&gt; data, count, dictionary, reverse_dictionary = \</span>
<span class="sd">                tl.nlp.build_words_dataset(words, vocabulary_size, True)</span>
<span class="sd">    &gt;&gt;&gt; analogy_questions = tl.nlp.read_analogies_file( \</span>
<span class="sd">                eval_file=&#39;questions-words.txt&#39;, word2id=dictionary)</span>
<span class="sd">    &gt;&gt;&gt; print(analogy_questions)</span>
<span class="sd">    ... [[ 3068  1248  7161  1581]</span>
<span class="sd">    ... [ 3068  1248 28683  5642]</span>
<span class="sd">    ... [ 3068  1248  3878   486]</span>
<span class="sd">    ... ...,</span>
<span class="sd">    ... [ 1216  4309 19982 25506]</span>
<span class="sd">    ... [ 1216  4309  3194  8650]</span>
<span class="sd">    ... [ 1216  4309   140   312]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">questions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">questions_skipped</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">eval_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">analogy_f</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">analogy_f</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">b</span><span class="s2">&quot;:&quot;</span><span class="p">):</span>  <span class="c1"># Skip comments.</span>
                <span class="k">continue</span>
          <span class="n">words</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">b</span><span class="s2">&quot; &quot;</span><span class="p">)</span>  <span class="c1"># lowercase</span>
          <span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
          <span class="k">if</span> <span class="bp">None</span> <span class="ow">in</span> <span class="n">ids</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
              <span class="n">questions_skipped</span> <span class="o">+=</span> <span class="mi">1</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">questions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ids</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Eval analogy file: &quot;</span><span class="p">,</span> <span class="n">eval_file</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Questions: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">questions</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Skipped: &quot;</span><span class="p">,</span> <span class="n">questions_skipped</span><span class="p">)</span>
    <span class="n">analogy_questions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">questions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">analogy_questions</span></div>

<div class="viewcode-block" id="build_vocab"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.build_vocab">[docs]</a><span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build vocabulary.</span>
<span class="sd">        Given the context in list format</span>
<span class="sd">        Return the vocabulary, which is a dictionary for word to id.</span>
<span class="sd">        e.g. {&#39;campbell&#39;: 2587, &#39;atlantic&#39;: 2247, &#39;aoun&#39;: 6746 .... }</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : a list of string</span>
<span class="sd">        the context in list format</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    word_to_id : a dictionary</span>
<span class="sd">        mapping words to unique IDs.</span>
<span class="sd">        e.g. {&#39;campbell&#39;: 2587, &#39;atlantic&#39;: 2247, &#39;aoun&#39;: 6746 .... }</span>

<span class="sd">    Code References</span>
<span class="sd">    ---------------</span>
<span class="sd">    `tensorflow.models.rnn.ptb.reader &lt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb&gt;`_</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; data_path = os.getcwd() + &#39;/simple-examples/data&#39;</span>
<span class="sd">    &gt;&gt;&gt; train_path = os.path.join(data_path, &quot;ptb.train.txt&quot;)</span>
<span class="sd">    &gt;&gt;&gt; word_to_id = build_vocab(read_txt_words(train_path))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># data = _read_words(filename)</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># print(&#39;counter&#39;, counter)   # dictionary for the occurrence number of each word, e.g. &#39;banknote&#39;: 1, &#39;photography&#39;: 1, &#39;kia&#39;: 1</span>
    <span class="n">count_pairs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="c1"># print(&#39;count_pairs&#39;,count_pairs)  # convert dictionary to list of tuple, e.g. (&#39;ssangyong&#39;, 1), (&#39;swapo&#39;, 1), (&#39;wachter&#39;, 1)</span>
    <span class="n">words</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">count_pairs</span><span class="p">))</span>
    <span class="n">word_to_id</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))))</span>
    <span class="c1"># print(words)    # list of words</span>
    <span class="c1"># print(word_to_id) # dictionary for word to id, e.g. &#39;campbell&#39;: 2587, &#39;atlantic&#39;: 2247, &#39;aoun&#39;: 6746</span>
    <span class="k">return</span> <span class="n">word_to_id</span></div>

<div class="viewcode-block" id="build_reverse_dictionary"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.build_reverse_dictionary">[docs]</a><span class="k">def</span> <span class="nf">build_reverse_dictionary</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a dictionary for converting word to integer id.</span>
<span class="sd">    Returns a reverse dictionary for converting a id to word.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">word_to_id</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">word_to_id</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">reverse_dictionary</span></div>

<div class="viewcode-block" id="build_words_dataset"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.build_words_dataset">[docs]</a><span class="k">def</span> <span class="nf">build_words_dataset</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">printable</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">unk_key</span> <span class="o">=</span> <span class="s1">&#39;UNK&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build the words dictionary and replace rare words with &#39;UNK&#39; token.</span>
<span class="sd">    The most common word has the smallest integer id.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    words : a list of string or byte</span>
<span class="sd">        The context in list format. You may need to do preprocessing on the words,</span>
<span class="sd">        such as lower case, remove marks etc.</span>
<span class="sd">    vocabulary_size : an int</span>
<span class="sd">        The maximum vocabulary size, limiting the vocabulary size.</span>
<span class="sd">        Then the script replaces rare words with &#39;UNK&#39; token.</span>
<span class="sd">    printable : boolen</span>
<span class="sd">        Whether to print the read vocabulary size of the given words.</span>
<span class="sd">    unk_key : a string</span>
<span class="sd">        Unknown words = unk_key</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    data : a list of integer</span>
<span class="sd">        The context in a list of ids</span>
<span class="sd">    count : a list of tuple and list</span>
<span class="sd">        count[0] is a list : the number of rare words</span>
<span class="sd">        count[1:] are tuples : the number of occurrence of each word</span>
<span class="sd">        e.g. [[&#39;UNK&#39;, 418391], (b&#39;the&#39;, 1061396), (b&#39;of&#39;, 593677),</span>
<span class="sd">                                        (b&#39;and&#39;, 416629), (b&#39;one&#39;, 411764)]</span>
<span class="sd">    dictionary : a dictionary</span>
<span class="sd">        word_to_id, mapping words to unique IDs.</span>
<span class="sd">    reverse_dictionary : a dictionary</span>
<span class="sd">        id_to_word, mapping id to unique word.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; words = tl.files.load_matt_mahoney_text8_dataset()</span>
<span class="sd">    &gt;&gt;&gt; vocabulary_size = 50000</span>
<span class="sd">    &gt;&gt;&gt; data, count, dictionary, reverse_dictionary = \</span>
<span class="sd">    ...     tl.nlp.build_words_dataset(words, vocabulary_size)</span>

<span class="sd">    Code References</span>
<span class="sd">    -----------------</span>
<span class="sd">    `tensorflow/examples/tutorials/word2vec/word2vec_basic.py &lt;https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/word2vec/word2vec_basic.py&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">collections</span>
    <span class="n">count</span> <span class="o">=</span> <span class="p">[[</span><span class="n">unk_key</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">count</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">count</span><span class="p">:</span>
        <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">unk_count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># dictionary[&#39;UNK&#39;]</span>
            <span class="n">unk_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">count</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">unk_count</span>
    <span class="n">reverse_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">printable</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Real vocabulary size    </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Limited vocabulary size {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">&gt;=</span> <span class="n">vocabulary_size</span> <span class="p">,</span> \
            <span class="s2">&quot;the limited vocabulary_size must be less than or equal to the read vocabulary_size&quot;</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">,</span> <span class="n">reverse_dictionary</span></div>

<div class="viewcode-block" id="words_to_word_ids"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.words_to_word_ids">[docs]</a><span class="k">def</span> <span class="nf">words_to_word_ids</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">word_to_id</span><span class="p">,</span> <span class="n">unk_key</span> <span class="o">=</span> <span class="s1">&#39;UNK&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a context (words) in list format and the vocabulary,</span>
<span class="sd">    Returns a list of IDs to represent the context.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : a list of string or byte</span>
<span class="sd">        the context in list format</span>
<span class="sd">    word_to_id : a dictionary</span>
<span class="sd">        mapping words to unique IDs.</span>
<span class="sd">    unk_key : a string</span>
<span class="sd">        Unknown words = unk_key</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    A list of IDs to represent the context.</span>

<span class="sd">    Example</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; words = tl.files.load_matt_mahoney_text8_dataset()</span>
<span class="sd">    &gt;&gt;&gt; vocabulary_size = 50000</span>
<span class="sd">    &gt;&gt;&gt; data, count, dictionary, reverse_dictionary = \</span>
<span class="sd">    ...         tl.nlp.build_words_dataset(words, vocabulary_size, True)</span>
<span class="sd">    &gt;&gt;&gt; context = [b&#39;hello&#39;, b&#39;how&#39;, b&#39;are&#39;, b&#39;you&#39;]</span>
<span class="sd">    &gt;&gt;&gt; ids = tl.nlp.words_to_word_ids(words, dictionary)</span>
<span class="sd">    &gt;&gt;&gt; context = tl.nlp.word_ids_to_words(ids, reverse_dictionary)</span>
<span class="sd">    &gt;&gt;&gt; print(ids)</span>
<span class="sd">    ... [6434, 311, 26, 207]</span>
<span class="sd">    &gt;&gt;&gt; print(context)</span>
<span class="sd">    ... [b&#39;hello&#39;, b&#39;how&#39;, b&#39;are&#39;, b&#39;you&#39;]</span>

<span class="sd">    Code References</span>
<span class="sd">    ---------------</span>
<span class="sd">    `tensorflow.models.rnn.ptb.reader &lt;https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if isinstance(data[0], six.string_types):</span>
    <span class="c1">#     print(type(data[0]))</span>
    <span class="c1">#     # exit()</span>
    <span class="c1">#     print(data[0])</span>
    <span class="c1">#     print(word_to_id)</span>
    <span class="c1">#     return [word_to_id[str(word)] for word in data]</span>
    <span class="c1"># else:</span>


    <span class="n">word_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word_to_id</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">word_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_to_id</span><span class="p">[</span><span class="n">unk_key</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">word_ids</span></div>
    <span class="c1"># return [word_to_id[word] for word in data]    # this one</span>

    <span class="c1"># if isinstance(data[0], str):</span>
    <span class="c1">#     # print(&#39;is a string object&#39;)</span>
    <span class="c1">#     return [word_to_id[word] for word in data]</span>
    <span class="c1"># else:#if isinstance(s, bytes):</span>
    <span class="c1">#     # print(&#39;is a unicode object&#39;)</span>
    <span class="c1">#     # print(data[0])</span>
    <span class="c1">#     return [word_to_id[str(word)] f</span>

<div class="viewcode-block" id="word_ids_to_words"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.word_ids_to_words">[docs]</a><span class="k">def</span> <span class="nf">word_ids_to_words</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">id_to_word</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a context (ids) in list format and the vocabulary,</span>
<span class="sd">    Returns a list of words to represent the context.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : a list of integer</span>
<span class="sd">        the context in list format</span>
<span class="sd">    id_to_word : a dictionary</span>
<span class="sd">        mapping id to unique word.</span>

<span class="sd">    Returns</span>
<span class="sd">    --------</span>
<span class="sd">    A list of string or byte to represent the context.</span>

<span class="sd">    Examples</span>
<span class="sd">    ---------</span>
<span class="sd">    see words_to_word_ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">id_to_word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span></div>

<div class="viewcode-block" id="save_vocab"><a class="viewcode-back" href="../../modules/nlp.html#tensorlayer.nlp.save_vocab">[docs]</a><span class="k">def</span> <span class="nf">save_vocab</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;vocab.txt&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Save the vocabulary to a file so the model can be reloaded.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    count : a list of tuple and list</span>
<span class="sd">        count[0] is a list : the number of rare words</span>
<span class="sd">        count[1:] are tuples : the number of occurrence of each word</span>
<span class="sd">        e.g. [[&#39;UNK&#39;, 418391], (b&#39;the&#39;, 1061396), (b&#39;of&#39;, 593677),</span>
<span class="sd">                                        (b&#39;and&#39;, 416629), (b&#39;one&#39;, 411764)]</span>

<span class="sd">    Examples</span>
<span class="sd">    ---------</span>
<span class="sd">    &gt;&gt;&gt; words = tl.files.load_matt_mahoney_text8_dataset()</span>
<span class="sd">    &gt;&gt;&gt; vocabulary_size = 50000</span>
<span class="sd">    &gt;&gt;&gt; data, count, dictionary, reverse_dictionary = \</span>
<span class="sd">    ...     tl.nlp.build_words_dataset(words, vocabulary_size, True)</span>
<span class="sd">    &gt;&gt;&gt; tl.nlp.save_vocab(count, name=&#39;vocab_text8.txt&#39;)</span>
<span class="sd">    &gt;&gt;&gt; vocab_text8.txt</span>
<span class="sd">    ... UNK 418391</span>
<span class="sd">    ... the 1061396</span>
<span class="sd">    ... of 593677</span>
<span class="sd">    ... and 416629</span>
<span class="sd">    ... one 411764</span>
<span class="sd">    ... in 372201</span>
<span class="sd">    ... a 325873</span>
<span class="sd">    ... to 316376</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pwd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pwd</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">):</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">as_text</span><span class="p">(</span><span class="n">count</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]),</span> <span class="n">count</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%d</span><span class="s2"> vocab saved to </span><span class="si">%s</span><span class="s2"> in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">pwd</span><span class="p">))</span></div>

<span class="c1">## Advanced functions for translation</span>
<span class="k">def</span> <span class="nf">basic_tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">_WORD_SPLIT</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">b</span><span class="s2">&quot;([.,!?</span><span class="se">\&quot;</span><span class="s2">&#39;:;)(])&quot;</span><span class="p">)):</span>
  <span class="sd">&quot;&quot;&quot;Very basic tokenizer: split the sentence into a list of tokens.</span>

<span class="sd">  Parameters</span>
<span class="sd">  -----------</span>
<span class="sd">  sentence : tensorflow.python.platform.gfile.GFile Object</span>
<span class="sd">  _WORD_SPLIT : regular expression for word spliting.</span>

<span class="sd">  see create_vocabulary</span>

<span class="sd">  Examples</span>
<span class="sd">  --------</span>
<span class="sd">  &gt;&gt;&gt; from tensorflow.python.platform import gfile</span>
<span class="sd">  &gt;&gt;&gt; train_path = &quot;wmt/giga-fren.release2&quot;</span>
<span class="sd">  &gt;&gt;&gt; with gfile.GFile(train_path + &quot;.en&quot;, mode=&quot;rb&quot;) as f:</span>
<span class="sd">  &gt;&gt;&gt;    for line in f:</span>
<span class="sd">  &gt;&gt;&gt;       tokens = tl.nlp.basic_tokenizer(line)</span>
<span class="sd">  &gt;&gt;&gt;       print(tokens)</span>
<span class="sd">  &gt;&gt;&gt;       exit()</span>
<span class="sd">  ... [b&#39;Changing&#39;, b&#39;Lives&#39;, b&#39;|&#39;, b&#39;Changing&#39;, b&#39;Society&#39;, b&#39;|&#39;, b&#39;How&#39;,</span>
<span class="sd">  ...   b&#39;It&#39;, b&#39;Works&#39;, b&#39;|&#39;, b&#39;Technology&#39;, b&#39;Drives&#39;, b&#39;Change&#39;, b&#39;Home&#39;,</span>
<span class="sd">  ...   b&#39;|&#39;, b&#39;Concepts&#39;, b&#39;|&#39;, b&#39;Teachers&#39;, b&#39;|&#39;, b&#39;Search&#39;, b&#39;|&#39;, b&#39;Overview&#39;,</span>
<span class="sd">  ...   b&#39;|&#39;, b&#39;Credits&#39;, b&#39;|&#39;, b&#39;HHCC&#39;, b&#39;Web&#39;, b&#39;|&#39;, b&#39;Reference&#39;, b&#39;|&#39;,</span>
<span class="sd">  ...   b&#39;Feedback&#39;, b&#39;Virtual&#39;, b&#39;Museum&#39;, b&#39;of&#39;, b&#39;Canada&#39;, b&#39;Home&#39;, b&#39;Page&#39;]</span>

<span class="sd">  References</span>
<span class="sd">  ----------</span>
<span class="sd">  Code from /tensorflow/models/rnn/translation/data_utils.py</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">space_separated_fragment</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="n">words</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">_WORD_SPLIT</span><span class="p">,</span> <span class="n">space_separated_fragment</span><span class="p">))</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">create_vocabulary</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">max_vocabulary_size</span><span class="p">,</span>
                      <span class="n">tokenizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalize_digits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">_DIGIT_RE</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">br</span><span class="s2">&quot;\d&quot;</span><span class="p">),</span>
                      <span class="n">_START_VOCAB</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="s2">&quot;_PAD&quot;</span><span class="p">,</span> <span class="n">b</span><span class="s2">&quot;_GO&quot;</span><span class="p">,</span> <span class="n">b</span><span class="s2">&quot;_EOS&quot;</span><span class="p">,</span> <span class="n">b</span><span class="s2">&quot;_UNK&quot;</span><span class="p">]):</span>
  <span class="sd">&quot;&quot;&quot;Create vocabulary file (if it does not exist yet) from data file.</span>

<span class="sd">  Data file is assumed to contain one sentence per line. Each sentence is</span>
<span class="sd">  tokenized and digits are normalized (if normalize_digits is set).</span>
<span class="sd">  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.</span>
<span class="sd">  We write it to vocabulary_path in a one-token-per-line format, so that later</span>
<span class="sd">  token in the first line gets id=0, second line gets id=1, and so on.</span>

<span class="sd">  Parameters</span>
<span class="sd">  -----------</span>
<span class="sd">    vocabulary_path : path where the vocabulary will be created.</span>
<span class="sd">    data_path : data file that will be used to create vocabulary.</span>
<span class="sd">    max_vocabulary_size : limit on the size of the created vocabulary.</span>
<span class="sd">    tokenizer : a function to use to tokenize each data sentence;</span>
<span class="sd">        if None, basic_tokenizer will be used.</span>
<span class="sd">    normalize_digits : Boolean</span>
<span class="sd">        if true, all digits are replaced by 0s.</span>

<span class="sd">  References</span>
<span class="sd">  ----------</span>
<span class="sd">  Code from /tensorflow/models/rnn/translation/data_utils.py</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">gfile</span><span class="o">.</span><span class="n">Exists</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Creating vocabulary </span><span class="si">%s</span><span class="s2"> from data </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">vocabulary_path</span><span class="p">,</span> <span class="n">data_path</span><span class="p">))</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">with</span> <span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">counter</span> <span class="o">%</span> <span class="mi">100000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="k">print</span><span class="p">(</span><span class="s2">&quot;  processing line </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">counter</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">line</span><span class="p">)</span> <span class="k">if</span> <span class="n">tokenizer</span> <span class="k">else</span> <span class="n">basic_tokenizer</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
          <span class="n">word</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">_DIGIT_RE</span><span class="p">,</span> <span class="n">b</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">if</span> <span class="n">normalize_digits</span> <span class="k">else</span> <span class="n">w</span>
          <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">vocab_list</span> <span class="o">=</span> <span class="n">_START_VOCAB</span> <span class="o">+</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">vocab</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_vocabulary_size</span><span class="p">:</span>
        <span class="n">vocab_list</span> <span class="o">=</span> <span class="n">vocab_list</span><span class="p">[:</span><span class="n">max_vocabulary_size</span><span class="p">]</span>
      <span class="k">with</span> <span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">vocab_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">vocab_list</span><span class="p">:</span>
          <span class="n">vocab_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">w</span> <span class="o">+</span> <span class="n">b</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary </span><span class="si">%s</span><span class="s2"> from data </span><span class="si">%s</span><span class="s2"> exists&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">vocabulary_path</span><span class="p">,</span> <span class="n">data_path</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">initialize_vocabulary</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Initialize vocabulary from file, return the word_to_id (dictionary)</span>
<span class="sd">  and id_to_word (list).</span>

<span class="sd">  We assume the vocabulary is stored one-item-per-line, so a file:\n</span>
<span class="sd">    dog\n</span>
<span class="sd">    cat\n</span>
<span class="sd">  will result in a vocabulary {&quot;dog&quot;: 0, &quot;cat&quot;: 1}, and this function will</span>
<span class="sd">  also return the reversed-vocabulary [&quot;dog&quot;, &quot;cat&quot;].</span>

<span class="sd">  Parameters</span>
<span class="sd">  -----------</span>
<span class="sd">    vocabulary_path : path to the file containing the vocabulary.</span>

<span class="sd">  Returns</span>
<span class="sd">  --------</span>
<span class="sd">    a pair: the vocabulary (a dictionary mapping string to integers), and</span>
<span class="sd">    the reversed vocabulary (a list, which reverses the vocabulary mapping).</span>

<span class="sd">    vocab : a dictionary</span>
<span class="sd">        Word to id.</span>
<span class="sd">    rev_vocab : a list</span>
<span class="sd">        Id to word.</span>

<span class="sd">  Examples</span>
<span class="sd">  --------</span>
<span class="sd">  ... Assume &#39;test&#39; contains</span>
<span class="sd">  ... dog</span>
<span class="sd">  ... cat</span>
<span class="sd">  ... bird</span>
<span class="sd">  &gt;&gt;&gt; vocab, rev_vocab = tl.nlp.initialize_vocabulary(&quot;test&quot;)</span>
<span class="sd">  &gt;&gt;&gt; print(vocab)</span>
<span class="sd">  &gt;&gt;&gt; {b&#39;cat&#39;: 1, b&#39;dog&#39;: 0, b&#39;bird&#39;: 2}</span>
<span class="sd">  &gt;&gt;&gt; print(rev_vocab)</span>
<span class="sd">  &gt;&gt;&gt; [b&#39;dog&#39;, b&#39;cat&#39;, b&#39;bird&#39;]</span>

<span class="sd">  Raises</span>
<span class="sd">  -------</span>
<span class="sd">    ValueError: if the provided vocabulary_path does not exist.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">gfile</span><span class="o">.</span><span class="n">Exists</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">):</span>
    <span class="n">rev_vocab</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="n">rev_vocab</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">())</span>
    <span class="n">rev_vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">rev_vocab</span><span class="p">]</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rev_vocab</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">rev_vocab</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Vocabulary file </span><span class="si">%s</span><span class="s2"> not found.&quot;</span><span class="p">,</span> <span class="n">vocabulary_path</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sentence_to_token_ids</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span>
                          <span class="n">tokenizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalize_digits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                          <span class="n">UNK_ID</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">_DIGIT_RE</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">br</span><span class="s2">&quot;\d&quot;</span><span class="p">)):</span>
  <span class="sd">&quot;&quot;&quot;Convert a string to list of integers representing token-ids.</span>

<span class="sd">  For example, a sentence &quot;I have a dog&quot; may become tokenized into</span>
<span class="sd">  [&quot;I&quot;, &quot;have&quot;, &quot;a&quot;, &quot;dog&quot;] and with vocabulary {&quot;I&quot;: 1, &quot;have&quot;: 2,</span>
<span class="sd">  &quot;a&quot;: 4, &quot;dog&quot;: 7&quot;} this function will return [1, 2, 4, 7].</span>

<span class="sd">  Parameters</span>
<span class="sd">  -----------</span>
<span class="sd">    sentence :  tensorflow.python.platform.gfile.GFile Object</span>
<span class="sd">        The sentence in bytes format to convert to token-ids.\n</span>
<span class="sd">        see basic_tokenizer(), data_to_token_ids()</span>
<span class="sd">    vocabulary : a dictionary mapping tokens to integers.</span>
<span class="sd">    tokenizer : a function to use to tokenize each sentence;</span>
<span class="sd">        If None, basic_tokenizer will be used.</span>
<span class="sd">    normalize_digits : Boolean</span>
<span class="sd">        If true, all digits are replaced by 0s.</span>

<span class="sd">  Returns</span>
<span class="sd">  --------</span>
<span class="sd">    A list of integers, the token-ids for the sentence.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">if</span> <span class="n">tokenizer</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">basic_tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">normalize_digits</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">UNK_ID</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
  <span class="c1"># Normalize digits by 0 before looking words up in the vocabulary.</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">_DIGIT_RE</span><span class="p">,</span> <span class="n">b</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">UNK_ID</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">data_to_token_ids</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">target_path</span><span class="p">,</span> <span class="n">vocabulary_path</span><span class="p">,</span>
                      <span class="n">tokenizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">normalize_digits</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">UNK_ID</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">_DIGIT_RE</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">br</span><span class="s2">&quot;\d&quot;</span><span class="p">)):</span>
  <span class="sd">&quot;&quot;&quot;Tokenize data file and turn into token-ids using given vocabulary file.</span>

<span class="sd">  This function loads data line-by-line from data_path, calls the above</span>
<span class="sd">  sentence_to_token_ids, and saves the result to target_path. See comment</span>
<span class="sd">  for sentence_to_token_ids on the details of token-ids format.</span>

<span class="sd">  Parameters</span>
<span class="sd">  -----------</span>
<span class="sd">    data_path: path to the data file in one-sentence-per-line format.</span>
<span class="sd">    target_path: path where the file with token-ids will be created.</span>
<span class="sd">    vocabulary_path: path to the vocabulary file.</span>
<span class="sd">    tokenizer: a function to use to tokenize each sentence;</span>
<span class="sd">      if None, basic_tokenizer will be used.</span>
<span class="sd">    normalize_digits: Boolean; if true, all digits are replaced by 0s.</span>

<span class="sd">  References</span>
<span class="sd">  ----------</span>
<span class="sd">  Code from /tensorflow/models/rnn/translation/data_utils.py</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">gfile</span><span class="o">.</span><span class="n">Exists</span><span class="p">(</span><span class="n">target_path</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Tokenizing data in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">data_path</span><span class="p">)</span>
    <span class="n">vocab</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">initialize_vocabulary</span><span class="p">(</span><span class="n">vocabulary_path</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">data_file</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">target_path</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tokens_file</span><span class="p">:</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">data_file</span><span class="p">:</span>
          <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
          <span class="k">if</span> <span class="n">counter</span> <span class="o">%</span> <span class="mi">100000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;  tokenizing line </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">counter</span><span class="p">)</span>
          <span class="n">token_ids</span> <span class="o">=</span> <span class="n">sentence_to_token_ids</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span>
                                            <span class="n">normalize_digits</span><span class="p">,</span> <span class="n">UNK_ID</span><span class="o">=</span><span class="n">UNK_ID</span><span class="p">,</span>
                                            <span class="n">_DIGIT_RE</span><span class="o">=</span><span class="n">_DIGIT_RE</span><span class="p">)</span>
          <span class="n">tokens_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">token_ids</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Target path </span><span class="si">%s</span><span class="s2"> exists&quot;</span> <span class="o">%</span> <span class="n">target_path</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, TensorLayer contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>